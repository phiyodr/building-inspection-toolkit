{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "multi-target.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNC8OQ0kr6R7JAgkACRYApz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/phiyodr/building-inspection-toolkit/blob/master/examples/colab/multi_target.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# dacl-demo\n",
        "\n",
        "In this demo-notebook we will be importing a model for **Multi-Target Classification** of damage on reinforced concrete structures. \n",
        "All steps for feeding the model with real image data of damage will be examined as follows:\n",
        "\n",
        "1. ***Create the DaclNet-class:*** For being able to instantiate a model, it's class, named DaclNet, has to be created. Currently there are three architectures available based on: ResNet50, EfficientNetV1-B0 and MobileNetV3-large.\n",
        "2. ***Preprocess the image***: Before feeding the model we need to prepare the image.\n",
        "3. ***Feed the model***: Finally, feed the dacl-model with tasty preprocessed image data.\n",
        "4. ***Analyze the results***: Analyze the dacl-model's results and try to interpret them.\n",
        "\n",
        "\n",
        "<img src='https://dacl.ai/assets/DACL_pixel.png' width=300px>"
      ],
      "metadata": {
        "id": "MPnROVTWhUs4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports\n",
        "Initially, we have to install and import the modules we want to use."
      ],
      "metadata": {
        "id": "ma5Y-NBvuGzP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os \n",
        "import random\n",
        "from pathlib import Path\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "from torchvision import models\n",
        "\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import json\n",
        "\n",
        "%pip install efficientnet_pytorch\n",
        "from efficientnet_pytorch import EfficientNet\n",
        "from efficientnet_pytorch.utils import MemoryEfficientSwish\n",
        "\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gAyh7FqQuOfo",
        "outputId": "8bf16233-22a7-4672-e8dc-ac1c32e0c71a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting efficientnet_pytorch\n",
            "  Downloading efficientnet_pytorch-0.7.1.tar.gz (21 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from efficientnet_pytorch) (1.10.0+cu111)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->efficientnet_pytorch) (3.10.0.2)\n",
            "Building wheels for collected packages: efficientnet-pytorch\n",
            "  Building wheel for efficientnet-pytorch (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for efficientnet-pytorch: filename=efficientnet_pytorch-0.7.1-py3-none-any.whl size=16446 sha256=c3f25d46aca625a1d180d9d5f4ffe8495590fb55ceaad08521d672cf5af9570f\n",
            "  Stored in directory: /root/.cache/pip/wheels/0e/cc/b2/49e74588263573ff778da58cc99b9c6349b496636a7e165be6\n",
            "Successfully built efficientnet-pytorch\n",
            "Installing collected packages: efficientnet-pytorch\n",
            "Successfully installed efficientnet-pytorch-0.7.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Instantiate the dacl-model\n",
        "\n",
        "First, we need to define the model with the DaclNet class."
      ],
      "metadata": {
        "id": "Eu26fnUmuYaK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dict to find the suiting EfficientNet model according to the resolution of the input-images:\n",
        "efnet_dict = {'b0': 224, 'b1': 240, 'b2': 260, 'b3': 300,   \n",
        "              'b4': 380, 'b5': 456, 'b6': 528, 'b7': 600    \n",
        "             }\n",
        "\n",
        "class DaclNet(nn.Module):\n",
        "    def __init__(self, base_name, resolution, hidden_layers, num_class, drop_prob=0.2, freeze_base=True):\n",
        "        ''' \n",
        "        Builds a network separated into a base model and classifier with arbitrary hidden layers.\n",
        "        \n",
        "        Attributes\n",
        "        ---------\n",
        "        base_name: string, basemodel for the NN\n",
        "        resolution: resolution of the input-images, example: 224, 240...(look efnet_dic), Only needed for EfficientNet\n",
        "        hidden_layers: list of integers, the sizes of the hidden layers\n",
        "        drop_prob: float, dropout probability\n",
        "        freeze_base: boolean, choose if you want to freeze the parameters of the base model\n",
        "        num_class: integer, size of the output layer according to the number of classes\n",
        "\n",
        "        Example\n",
        "        ---------\n",
        "        model = Network(base_name='efficientnet', resolution=224, hidden_layers=[32,16], num_class=6, drop_prob=0.2, freeze_base=True)\n",
        "        \n",
        "        Note\n",
        "        ---------\n",
        "        -print(efficientnet) -> Last module: (_swish): MemoryEfficientSwish() and the last fc-layers\n",
        "         This activation won't be called during forward due to: \"self.base.extract_features\"! No activation of last layer!\n",
        "        '''\n",
        "        super(DaclNet, self).__init__()\n",
        "        # basemodel\n",
        "        self.base_name = base_name\n",
        "        self.resolution = resolution\n",
        "        self.hidden_layers = hidden_layers\n",
        "        self.freeze_base = freeze_base\n",
        "\n",
        "        if self.base_name == 'mobilenet':\n",
        "            base = models.mobilenet_v3_large(pretrained=True) \n",
        "            modules = list(base.children())[:-1] \n",
        "            self.base = nn.Sequential(*modules)\n",
        "            # for pytorch model:\n",
        "            if hidden_layers:\n",
        "                self.classifier = nn.ModuleList([nn.Linear(base.classifier[0].in_features, self.hidden_layers[0])]) \n",
        "            else:\n",
        "                self.classifier = nn.Linear(base.classifier[0].in_features, num_class)\n",
        "\n",
        "            self.activation = nn.Hardswish()\n",
        "\n",
        "        elif self.base_name == 'resnet':\n",
        "            base = models.resnet50(pretrained=True) \n",
        "            modules = list(base.children())[:-1]\n",
        "            self.base = nn.Sequential(*modules)\n",
        "            if self.hidden_layers:\n",
        "                self.classifier = nn.ModuleList([nn.Linear(base.fc.in_features, self.hidden_layers[0])])\n",
        "            else:\n",
        "                self.classifier = nn.Linear(base.fc.in_features, num_class)   \n",
        "            self.activation = nn.ELU() \n",
        "\n",
        "        elif self.base_name == 'efficientnet':      \n",
        "            for ver in efnet_dict:\n",
        "                if efnet_dict[ver] == self.resolution:\n",
        "                    self.version = ver\n",
        "                    full_name = self.base_name+'-'+ver\n",
        "            self.base = EfficientNet.from_pretrained(model_name=full_name) \n",
        "            if self.hidden_layers:\n",
        "                self.classifier = nn.ModuleList([nn.Linear(self.base._fc.in_features, self.hidden_layers[0])])\n",
        "            else:\n",
        "                self.classifier = nn.Linear(self.base._fc.in_features, num_class)   \n",
        "            self.activation = MemoryEfficientSwish()\n",
        "        elif self.base_name == 'mobilenetv2':\n",
        "            base = models.mobilenet.mobilenet_v2(pretrained=True)\n",
        "            modules = list(base.children())[:-1]\n",
        "            self.base = nn.Sequential(*modules)\n",
        "            if hidden_layers:\n",
        "                self.classifier = nn.ModuleList([nn.Linear(base.classifier[1].in_features, self.hidden_layers[0])]) \n",
        "            else:\n",
        "                self.classifier = nn.Linear(base.classifier[1].in_features, num_class)\n",
        "            self.activation = nn.ReLU()\n",
        "        else:\n",
        "            raise NotImplementedError    \n",
        "        \n",
        "        # freeze the base\n",
        "        if self.freeze_base:\n",
        "            for param in self.base.parameters(): \n",
        "                param.requires_grad_(False)\n",
        "        \n",
        "        self.dropout = nn.Dropout(p=drop_prob, inplace=True)\n",
        "\n",
        "        # classifier\n",
        "        # Add a variable number of more hidden layers\n",
        "        if self.hidden_layers:\n",
        "            layer_sizes = zip(self.hidden_layers[:-1], self.hidden_layers[1:])        \n",
        "            self.classifier.extend([nn.Linear(h1, h2) for h1, h2 in layer_sizes])\n",
        "            # Add output layer to classifier\n",
        "            self.classifier.append(nn.Linear(self.hidden_layers[-1], num_class))\n",
        "        else:\n",
        "            pass\n",
        "        \n",
        "    def forward(self, input_batch):\n",
        "        ''' \n",
        "        Performs the feed-forward process for the input batch and return the logits\n",
        "\n",
        "        Arguments\n",
        "        ---------\n",
        "        input_batch: torch.Tensor, Multidimensional array holding elements of datatype: torch.float32, \n",
        "                     it's shape is: [1, 3, 224, 224] according to N x C x H x W,\n",
        "                     The input batch carries all pixel values from the images inside teh batch\n",
        "        Note\n",
        "        ---------\n",
        "        Every model uses 2d-Average-Pooling with output_size=1 after the feature extraction or rather before flattening.\n",
        "        The pooling layer of ResNet50 and MobileNetV3 was kept in the squential -> Doesn't have to be called in forward!\n",
        "        EffNet had to be implemented with the AdaptiveAvgpool2d in this forward function because of missing pooling when\n",
        "        calling: \"effnet.extract_features(input_batch)\"\n",
        "        Also mobilenetV2 needs the manually added pooling layer.\n",
        "\n",
        "        Returns\n",
        "        ---------\n",
        "        logits: torch.Tensor, shape: [1, num_class], datatype of elements: float\n",
        "        '''\n",
        "        # Check if model is one that needs Pooling layer and/or special feature extraction\n",
        "        if self.base_name in ['efficientnet', 'mobilenetv2']:\n",
        "            if self.base_name == 'efficientnet':\n",
        "                x = self.base.extract_features(input_batch)\n",
        "            else:\n",
        "                # For MobileNetV2\n",
        "                x= self.base(input_batch)\n",
        "            pool = nn.AdaptiveAvgPool2d(1)\n",
        "            x = pool(x)\n",
        "        else:\n",
        "            # For any other model don't additionally apply pooling:\n",
        "            x = self.base(input_batch)\n",
        "        \n",
        "        x = self.dropout(x)         # Originally only in EfficientNet a Dropout after feature extraction is added  \n",
        "        x = x.flatten(start_dim=1)\n",
        "        if self.hidden_layers:    \n",
        "            for i,each in enumerate(self.classifier):\n",
        "                # Put an activation function and dropout after each hidden layer\n",
        "                if i < len(self.classifier)-1:\n",
        "                    x = self.activation(each(x))\n",
        "                    x = self.dropout(x)\n",
        "                else:\n",
        "                    # Don't use an activation and dropout for the last layer\n",
        "                    logits = each(x)\n",
        "                    break\n",
        "        else:\n",
        "            logits = self.classifier(x)\n",
        "\n",
        "        return logits\n"
      ],
      "metadata": {
        "id": "b_dv_-sWussA"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Preprocessing and helper fucntions\n",
        "\n",
        "Before feeding the model, we need to prepare the image. \n",
        "We have to get an image transformed to a tensor with the shape *N x C x H x W* where *N* is the batch-size, *C* the color channels (RGB), *H* the height and *W* the width of the image. Also there's a function for selecting an arbitrary image and one to display our result next to the classified image."
      ],
      "metadata": {
        "id": "mOj4Cg8vu1sR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocessing-function:\n",
        "def process_img(img_path=None):\n",
        "\t''' \n",
        "\tScales, crops, and normalizes a PIL image for a PyTorch model,\n",
        "\treturns a Torch Tensor\n",
        "\tArgs: \n",
        "\t\tfilepath: \tfilepath of the image (string)\n",
        "\tExample: process_img('test/1/image_06743.jpg')\n",
        "\tReturns: torch.float32 of shape: [1, 3, 224, 224]\n",
        "\t'''\n",
        "\n",
        "\tif not img_path:\n",
        "\t\tprint('Parse the filename of the image!')\n",
        "\telse:\n",
        "\t\t#Parse image as PIL Image\n",
        "\t\timage = Image.open(img_path)\n",
        "\t\t# Setting Resize Parameters (width and height)\n",
        "\t\timage_ratio = image.height / image.width\n",
        "\t\tif  image.width < image.height  or image.width > image.height:\n",
        "\t\t\tif image.width < image.height:\n",
        "\t\t\t\tresize = (256, int(image_ratio * 256))\n",
        "\t\t\telse:\n",
        "\t\t\t\tresize = (int(256 / image_ratio), 256)\n",
        "\t\telse:\n",
        "\t\t\tresize = (256, 256)\n",
        "\t\t\n",
        "\t\t#Setting Crop parameters\n",
        "\t\tcrop_size = 224\n",
        "\t\tcrop_x = int((resize[0] - crop_size) / 2)\n",
        "\t\tcrop_y = int((resize[1] - crop_size) / 2)\n",
        "\t\tcrop_box = (crop_x, crop_y,crop_x + crop_size, crop_y+crop_size)\n",
        "\t  \t\n",
        "\t\t#Transformation\n",
        "\t\tpil_image = image.resize(resize)\n",
        "\t\tpil_image = pil_image.crop(crop_box)\n",
        "\t\tnp_image = np.array(pil_image)\n",
        "\t\tnp_image = (np_image/255 - [0.485, 0.456, 0.406]) / [0.229, 0.224, 0.225]\n",
        "\t\tnp_image = np_image.transpose(2,0,1)\n",
        "\t\timage = torch.from_numpy(np_image)\n",
        "\t\timage = image.unsqueeze_(0)\n",
        "\t\timage = image.type(torch.FloatTensor)\n",
        "\t\treturn image\n",
        "\n",
        "def rand_img_path(dir=''):\n",
        "  ''' \n",
        "  Returns an arbitrary path to an image file from a directory, also in its subfolders.\n",
        "  Args: dir(directory)\n",
        "  Example: rand_img_path('assets/DamageExamples')\n",
        "  '''\n",
        "  file = os.path.join(dir, random.choice(os.listdir(dir)));\n",
        "  if os.path.isdir(file):\n",
        "    return rand_img_path(file)\n",
        "  else:\n",
        "    return file\n",
        "\n",
        "def view_classify(img_path, result_dict, title=None, normalize=True):\n",
        "  ''' Function for viewing an image, its predicted classes and the probabilities\n",
        "      in a horizontal bar chart.\n",
        "      Parameters:\n",
        "      image_path - path to image you want to classify. You can take \n",
        "      random_image_path function so a random image from test-folder will be classified\n",
        "      result_dict -  result_dict returned by the predict function\n",
        "      Returns:\n",
        "      None - just displays the image next to the bar chart  \n",
        "  '''\n",
        "\n",
        "  result_list = list(result_dict.items())\n",
        "  result_list = sorted(result_list, reverse=False, key=lambda result: result[1])\n",
        "  cat_names = [x[0] for x in result_list]\n",
        "  ps = [x[1] for x in result_list]\n",
        "\n",
        "  fig, (ax1, ax2) = plt.subplots(figsize=(9,12), ncols=2)\n",
        "  ax1.imshow(plt.imread(img_path))\n",
        "  ax1.axis('off')\n",
        "  ax1.set_title(cat_names[-1])\n",
        "  ax2.barh(range(len(cat_names)), ps, align='center')\n",
        "  ax2.set_aspect(0.1)\n",
        "  ax2.set_yticks(np.arange(len(cat_names)))\n",
        "  ax2.set_yticklabels(cat_names, size='small')\n",
        "  ax2.set_title('Class Probability')\n",
        "  ax2.set_xlim(0, 1.1)\n",
        "\n",
        "  plt.tight_layout()\n",
        "\n",
        "\n",
        "# Get image and show it:\n",
        "img_dir = 'assets/DamageExamples'\n",
        "img_path = rand_img_path(img_dir)\n",
        "try:\n",
        "\timg = plt.imread(img_path)\n",
        "except:\n",
        "\tprint(\"ERROR: {} is no image file. Don't leave non-image files in the {} folder or change the img_dir!\".format(img_path, img_dir))\n",
        "plt.imshow(img)\n",
        "\n",
        "# Preprocess:\n",
        "img_proc = process_img(img_path)\n",
        "print(\"The datatype of the preprocessed image is: {} and it's shape is: {}\".format(img_proc.dtype, img_proc.shape))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        },
        "id": "-cevfAOQvCow",
        "outputId": "836f4e08-9f29-47e8-f10e-de0c873116d3"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-08e2a7779170>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;31m# Get image and show it:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0mimg_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'assets/DamageExamples'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m \u001b[0mimg_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrand_img_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-08e2a7779170>\u001b[0m in \u001b[0;36mrand_img_path\u001b[0;34m(dir)\u001b[0m\n\u001b[1;32m     48\u001b[0m   \u001b[0mExample\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mrand_img_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'assets/DamageExamples'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m   '''\n\u001b[0;32m---> 50\u001b[0;31m   \u001b[0mfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mrand_img_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'assets/DamageExamples'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Feed the dacl-model\n",
        "Finally, we can instantiate the mdoel, load a checkpoint of your choice and feed the model with preprocessed image data. You will see the predicted result under the following code cell.\n",
        "\n",
        "**Choose a checkpoint from the table inside the README**"
      ],
      "metadata": {
        "id": "IBTZXCRfvPu1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Choose which checkpoint/model you want to load from the table above:\n",
        "cp_name = 'Code_mobilev2_dacl.pth'\n",
        "\n",
        "# Load the checkpoint:\n",
        "cp = torch.load(Path('models/' + cp_name)) \n",
        "\n",
        "# Instantiate the model:\n",
        "model = DaclNet(base_name=cp['base'], resolution = cp['resolution'], hidden_layers=cp['hidden_layers'], \n",
        "\t\t\t\tdrop_prob=cp['drop_prob'], num_class=cp['num_class'])\n",
        "model.load_state_dict(cp['state_dict']) # Load the pre-trained wights into the model\n",
        "model.eval() # Set the model to eval-mode. No dropout and no autograd will be applied.\n",
        "\n",
        "# Now let's feed the dacl-model in order to  classify the preprocessed image that we imported at the beginning:\n",
        "logits = model(img_proc)\n",
        "\n",
        "# Apply sigmoid activation to get predictions:\n",
        "preds = torch.sigmoid(logits).float().squeeze(0)\n",
        "\n",
        "# Binarize results:\n",
        "threshold = .5 # Which threshold do you want to choose for binarization of predictions (for bikit .5 was chosen)?\n",
        "bin = np.array(preds > threshold, dtype=float)\n",
        "\n",
        "# In the cat_to_name file our damage-class-names are stored with the according position in the output vector:\n",
        "with open('cat_to_name.json', 'r') as f:\n",
        "\tcat_to_name = json.load(f)[cp['dataset']]\n",
        "\n",
        "# Output:\n",
        "# Make a dict with the predictions:\n",
        "preds_dict = {v:round(preds[int(k)].item(),2) for k,v in cat_to_name.items()}\n",
        "print('*'*10, 'Output', '*'*10)\n",
        "# print('This is the model you have just created:\\n', model)\n",
        "for k,v in preds_dict.items():\n",
        "\tif v > .5:\n",
        "\t\tprint('%s: %.2f%%' % (k,v*100)) \n",
        "\n",
        "# View the classified image and it's predictions:\n",
        "view_classify(img_path, preds_dict)"
      ],
      "metadata": {
        "id": "DBBytPTWvgSy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## InProgress: Analyze the results\n",
        "Analyze the dacl-model's results and try to interpret them. Here is planned to provide code for testing on test datasets from bikit."
      ],
      "metadata": {
        "id": "H7k2yrN7vwcz"
      }
    }
  ]
}